{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3004,"databundleVersionId":861823,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"execution_failed":"2025-08-02T18:06:21.010Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport optuna\nimport warnings\nimport time\n\n# Import models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nimport xgboost as xgb\nimport lightgbm as lgb\nimport catboost as cb\n\n# Import scikit-learn helpers\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate, cross_val_score\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.exceptions import ConvergenceWarning\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-02T18:06:21.010Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n\n# --- Load Data ---\nprint(\"Loading data...\")\ntrain_full = pd.read_csv('/kaggle/input/digit-recognizer/train.csv')\ntest_set = pd.read_csv('/kaggle/input/digit-recognizer/test.csv')\n\n# --- Prepare Full Training Data ---\nX_train_full = train_full.drop(columns=['label'])\ny_train_full = train_full['label']\n\n# Normalize pixel values\nX_train_full = X_train_full / 255.0\ntest_set = test_set / 255.0\nprint(\"Data loaded and normalized.\")\n\nprint(\"\\n--- Applying Feature Selection (Variance Threshold) ---\")\nprint(f\"Original number of features: {X_train_full.shape[1]}\")\nselector = VarianceThreshold(threshold=0.0)\nX_train_full = selector.fit_transform(X_train_full)\ntest_set = selector.transform(test_set)\nprint(f\"Features after removing zero-variance pixels: {X_train_full.shape[1]}\")\n\n# --- Create a smaller stratified subset for SVM tuning ---\n# This is crucial to get results in a reasonable time\nX_train, X_val, y_train, y_val = train_test_split(\n    X_train_full, y_train_full, test_size=0.2, random_state=42, stratify=y_train_full\n)\nX_train_svm, _, y_train_svm, _ = train_test_split(\n    X_train, y_train, train_size=8000, random_state=42, stratify=y_train\n)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-02T08:04:34.233Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\noptuna.logging.set_verbosity(optuna.logging.WARNING)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-02T08:04:34.234Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Objective Functions for Optuna ---\nprint(\"\\n\\n\" + \"=\"*60)\nprint(\"--- STAGE 1: Starting Fast Scan with Hold-Out Validation ---\")\nprint(\"=\"*60)\n# --- Corrected Objective Function for a More Robust Fast Scan ---\n\ndef objective_fast_scan(trial, model_name):\n    \"\"\"A single objective function for the fast scan with corrected multi-parameter search.\"\"\"\n    xt, yt, xv, yv = (X_train_svm, y_train_svm, X_val, y_val) if 'SVM' in model_name else (X_train, y_train, X_val, y_val)\n\n    if model_name == 'Logistic Regression':\n        # --- Corrected Conditional Logic for Solver and Penalty ---\n        solver = trial.suggest_categorical('solver', ['lbfgs', 'saga'])\n        params = {\n            'C': trial.suggest_float('C', 1e-3, 1e3, log=True),\n            'solver': solver,\n            'max_iter': 1000, # Increased for better convergence\n            'random_state': 42\n        }\n        # Suggest only valid penalties for the chosen solver\n        if solver == \"lbfgs\":\n            params['penalty'] = trial.suggest_categorical('penalty_lbfgs', ['l2', None])\n        else: # solver == \"saga\"\n            params['penalty'] = trial.suggest_categorical('penalty_saga', ['l1', 'l2', 'elasticnet', None])\n            # If elasticnet is chosen, we must also suggest an l1_ratio\n            if params['penalty'] == 'elasticnet':\n                params['l1_ratio'] = trial.suggest_float('l1_ratio', 0.0, 1.0)\n        \n        model = LogisticRegression(**params)\n\n    elif model_name == 'Naive Bayes':\n        var_smoothing = trial.suggest_float('var_smoothing', 1e-10, 1e-1, log=True)\n        model = GaussianNB(var_smoothing=var_smoothing)\n\n    elif model_name == 'Random Forest':\n        # --- Corrected Parameter Names and Choices ---\n        params = {\n            'n_estimators': trial.suggest_int('n_estimators', 50, 400),\n            'max_depth': trial.suggest_int('max_depth', 8, 30),\n            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 20),\n            'criterion': trial.suggest_categorical('criterion', ['gini', 'entropy']), # Corrected to list\n            'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n            'min_impurity_decrease': trial.suggest_float('min_impurity_decrease', 1e-7, 1e-2, log=True) # Corrected name and range\n        }\n        model = RandomForestClassifier(**params, random_state=42, n_jobs=-1)\n\n    elif model_name == 'XGBoost':\n        params = {\n            'n_estimators': trial.suggest_int('n_estimators', 100, 800),\n            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n            'max_depth': trial.suggest_int('max_depth', 3, 7),\n            'random_state': 42\n        }\n        model = xgb.XGBClassifier(**params)\n        \n    elif model_name == 'LightGBM':\n        params = {\n            'n_estimators': trial.suggest_int('n_estimators', 100, 800),\n            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n            'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n            'random_state': 42\n        }\n        model = lgb.LGBMClassifier(**params)\n        \n    elif model_name == 'CatBoost':\n        params = {\n            'iterations': trial.suggest_int('iterations', 100, 800),\n            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n            'depth': trial.suggest_int('depth', 4, 8),\n            'random_state': 42,\n            'verbose': 0\n        }\n        model = cb.CatBoostClassifier(**params)\n        \n    elif model_name == 'Linear SVM':\n        C = trial.suggest_float('C', 1e-3, 1e3, log=True)\n        model = SVC(C=C, kernel='linear', random_state=42)\n        \n    elif model_name == 'RBF SVM':\n        C = trial.suggest_float('C', 1e-1, 1e4, log=True)\n        gamma = trial.suggest_float('gamma', 1e-4, 1e-1, log=True)\n        model = SVC(C=C, gamma=gamma, kernel='rbf', random_state=42)\n\n    model.fit(xt, yt)\n    return model.score(xv, yv)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-02T08:04:34.234Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"N_TRIALS_FAST_SCAN = 10\nmodels_to_scan = [\"Logistic Regression\", \"Naive Bayes\", \"Random Forest\", \"XGBoost\", \"LightGBM\", \"CatBoost\", \"Linear SVM\", \"RBF SVM\"]\nfast_scan_results = []\n\nfor model_name in models_to_scan:\n    print(f\"\\n--- Scanning {model_name} ---\")\n    study = optuna.create_study(direction='maximize')\n    \n    # Let Optuna run all trials at once and show its own progress bar\n    study.optimize(\n        lambda trial: objective_fast_scan(trial, model_name),\n        n_trials=N_TRIALS_FAST_SCAN,\n        show_progress_bar=True  # This is the key argument\n    )\n    \n    fast_scan_results.append({\"Model\": model_name, \"Best Accuracy (Hold-Out)\": study.best_value})","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-02T08:04:34.234Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}